---
title: Variable selection and importance
date: last-modified
citeproc: true
bibliography: /home/danielschoenig/cloud/phd/literature/bibliographies/phd_all.json
csl: /home/danielschoenig/cloud/phd/literature/styles/apa.csl
format:
  pdf: 
    fig-pos: H
    fig-width: 7
    fig-dpi: 150
    fig-format: png
    documentclass: scrartcl
    papersize: letter
    mainfont: IBM Plex Serif
    sansfont: IBM Plex Sans
    monofont: IBM Plex Mono
    latex-tinytex: false
    pdf-engine: xelatex
    header-includes:
      - \usepackage{setspace}
      - \onehalfspacing
      - \usepackage{eulervm}
      - \usepackage{unicode-math}
      - \usepackage[format=plain]{caption}
      - \setkomafont{caption}{\footnotesize}
      - \setkomafont{captionlabel}{\footnotesize\bfseries}
---

# Variable importance in random forests

## Variable importance for correlated variables

We have used permutation variable importance [@Breiman2011], which measures
how much the prediction error of a random forest model increases, if
that specific variable is rendered meaningless. The performance of a
random forest can be expressed by how far, on average, the model
predictions are off from the observed truth. For example, if we observed
1, and the model predicts 0.8, the error for this observation will be
0.2. Averaging all these errors for the random forest gives the
*prediction error*. Now we can take a variable we are interested in and
shuffle its values (also called *permuting* or *noising* a variable).
The permuted variable does not carry any information, and we can feed it
to the model to generate predictions based on the permuted
variable[footnote]. As a result of the permutation, the prediction error
may change, and this change can be expressed as a percentage. For
example, if permuting a variable increases the prediction error from 0.2
to 0.3, it constitutes a 50% increase in prediction error. This
percentage increase in prediction error is the classic *variable
importance* measure of @Breiman2011. If a variable has a variable
importance of 10, it means that permuting the variable (i.e. removing
its information content) results in a 10% increase in prediction error.
Negative variable importance means that permuting the variable actually
reduces the prediction error, that is it improves the model.

A problem with variable importance measures arises if two (or more)
strongly correlated variables are present in the random forest. If
variables are strongly correlated, they present partly duplicated
information: once one of the variables is included in the model, the
second adds little additional information. This has consequences for
variable importance measures: We can imagine two variables that carry
very important information, but that are also highly correlated. If one
of the correlated variables is permuted, the predictions of the model
are not strongly affected, because the same information is still present
as within the other, remaining variable. This leads, for both variables,
to a low importance score. Only once both variables are removed
together, does the prediction error increase substantially. Now if the
goal is to select a small number of variables from all the variables in the
random forest, this creates a problem: if there is a group of correlated
variables, they have little chance of being selected even though the
information they carry may be more important than the information
containted in the variables with very high importance measures.

They only way to work around this problem is finding a random forest
model with a reduced number of variables, obtained by progressively
including new variables, or by excluding them from the full model.
However there are no established threshold at what point a smaller model
is "close enough" in performance to the best model, or at what point
adding additional variables merely leads to overfitting (i.e. fitting to
random "noise" patterns in the data). It is difficult to find these kind
of variables selection examples in the literature, as machine learning
models most often work with the full set of variables.


# Alternative approach: Bayesian model selection

## Selecting a subset of important variables

Rather than using random forests for variable selection, I suggest
adopting a Bayesian variable method, presented by 
